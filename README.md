# ğŸ§  Building LLMs from Scratch â€“ A 30-Day Journey

This repository guides you through the process of building a GPT-style **Large Language Model (LLM)** from scratch using PyTorch. The structure and approach are inspired by the book ***Build a Large Language Model (From Scratch)*** by **Sebastian Raschka**.

---

## ğŸ“˜ Reference Book

* **Title**: *Build a Large Language Model (From Scratch)*
* **Author**: Sebastian Raschka
* **Publisher**: Manning Publications
* **Link**: [manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)
* **Free Version**: [On Github Gist](https://gist.github.com/codewithdark-git/e204e6c06546f652e76ced9d479d914e)
* **Download PDF**: [PDF Version](https://raw.github.com/codewithdark-git/Building-LLMs-from-scratch/379208ccc204218f0ffc9114464b36d96a97505e/Building%20LLMs%20From%20Scratch.pdf)
* **Resourse**: [Check the Resourse From Where I learn](utils/resources.md)

---

## ğŸ—“ï¸ Weekly Curriculum Overview

### ğŸ”¹ Week 1: Foundations of Language Models

* Set up the environment and tools.
* Learn about tokenization, embeddings, and the idea of a "language model".
* Encode input/output sequences and build basic forward models.
* Understand unidirectional processing and causal language modeling.

### ğŸ”¹ Week 2: Building the Transformer Decoder

* Explore Transformer components: attention, multi-head attention, and positional encoding.
* Implement residual connections, normalization, and feedforward layers.
* Build a GPT-style decoder-only transformer architecture.

### ğŸ”¹ Week 3: Training and Dataset Handling

* Load and preprocess datasets like TinyShakespeare.
* Implement batch creation, context windows, and training routines.
* Use cross-entropy loss, optimizers, and learning rate schedulers.
* Monitor perplexity and improve generalization.

### ğŸ”¹ Week 4: Text Generation and Deployment

* Generate text using greedy, top-k, top-p, and temperature sampling.
* Evaluate and tune generation.
* Export and convert model for Hugging Face compatibility.
* Deploy via Hugging Face Hub and Gradio Space.

---

## ğŸ› ï¸ Getting Started

### Prerequisites

* Python 3.8+
* PyTorch
* NumPy
* Matplotlib
* JupyterLab or Notebooks
* Hugging Face libraries: `transformers`, `datasets`, `huggingface_hub`
* `gradio` for deployment

### Installation

```bash
git clone https://github.com/codewithdark-git/Building-LLMs-from-scratch.git
cd Building-LLMs-from-scratch
pip install -r requirements.txt
````

---

## ğŸ“ Project Structure

```
Building-LLMs-from-scratch/
â”œâ”€â”€ notebooks/            # Weekly learning notebooks
â”œâ”€â”€ models/               # Model architectures & checkpoints
â”œâ”€â”€ data/                 # Preprocessing and datasets
â”œâ”€â”€ hf_deploy/            # Hugging Face config & deployment scripts
â”œâ”€â”€ theoretical/          # Podcast & theoretical discussions
â”œâ”€â”€ utils/                # Helper scripts
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Hugging Face Deployment

This project includes:

* Scripts to convert the model for ğŸ¤— Transformers compatibility
* Uploading to Hugging Face Hub
* Launching an interactive demo on Hugging Face Spaces using Gradio

Youâ€™ll find detailed instructions inside the `hf_deploy/` folder.

---

## ğŸ“š Resources

* [Transformers Docs](https://huggingface.co/docs/transformers)
* [Hugging Face](https://huggingface.co)

---

## ğŸ“„ License

MIT License â€” see the `LICENSE` file for details.

